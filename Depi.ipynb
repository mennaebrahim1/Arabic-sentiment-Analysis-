{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbPfbcHKSPSU"
      },
      "source": [
        "# Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU5znv9Db7yD",
        "outputId": "63c46c21-f472-44a2-a93a-f4efe303e1d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!pip install tensorflow==2.13.0\\n!pip install keras==2.13.1\\n!pip install tensorflow-addons==0.16.1'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!pip install tensorflow==2.13.0\n",
        "!pip install keras==2.13.1\n",
        "!pip install tensorflow-addons==0.16.1\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf6x6_m4UGIE",
        "outputId": "97b9edd9-5d63-49cb-9ef7-13bca628a50c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!pip install googletrans\\n!pip install langdetect\\n!pip install emojis\\n!pip install emoji\\n!pip install pyarabic\\n!pip install nltk\\n!pip install keras_tuner\\n!pip install wordcloud\\n!pip install prettytable\\n!pip install arabic_reshaper\\n!pip install python-bidi\\n!pip install openpyxl'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!pip install googletrans\n",
        "!pip install langdetect\n",
        "!pip install emojis\n",
        "!pip install emoji\n",
        "!pip install pyarabic\n",
        "!pip install nltk\n",
        "!pip install keras_tuner\n",
        "!pip install wordcloud\n",
        "!pip install prettytable\n",
        "!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install openpyxl\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RnK7_ImkSj_",
        "outputId": "3803c71c-61a5-43b9-cc8a-ee4957f0f338"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\me513\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import csv\n",
        "import emojis\n",
        "import pyarabic.araby as araby\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOLwL5sXUmmq"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niO0MMTUUi_M",
        "outputId": "c0be7177-e8fc-47c8-fd55-e28ea4e33a59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32036, 1000)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "column_names = [\"review_description\", \"rating\"]\n",
        "train_data = pd.read_excel(r\"C:\\Users\\me513\\Downloads\\Depi-20241020T181025Z-001\\Depi\\train.xlsx\", names=column_names)\n",
        "test_data = pd.read_csv(r\"C:\\Users\\me513\\Downloads\\Depi-20241020T181025Z-001\\Depi\\test _no_label.csv\",encoding=\"utf-8\",dtype={'review_description': str})\n",
        "\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwonCNd-U7kb",
        "outputId": "568abf24-369f-4e1b-d45a-93e4de7fc893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "review_description    False\n",
            "rating                False\n",
            "dtype: bool\n",
            "0\n",
            "ID                    False\n",
            "review_description    False\n",
            "dtype: bool\n"
          ]
        }
      ],
      "source": [
        "train_data[train_data.isnull().any(axis=1)].head()\n",
        "test_data[train_data.isnull().any(axis=1)].head()\n",
        "print(np.sum(train_data.isnull().any(axis=1)))\n",
        "print(train_data.isnull().any(axis=0))\n",
        "\n",
        "print(np.sum(test_data.isnull().any(axis=1)))\n",
        "print(test_data.isnull().any(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAPeNgnZUWgg",
        "outputId": "3eb27ad6-1a3b-4af5-b5dc-b753625e53a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(                                      review_description  rating\n",
              " 0      شركه زباله و سواقين بتبرشم و مفيش حتي رقم للشك...      -1\n",
              " 1      خدمة الدفع عن طريق الكي نت توقفت عندي اصبح فقط...       1\n",
              " 2      تطبيق غبي و جاري حذفه ، عاملين اكواد خصم و لما...      -1\n",
              " 3      فعلا تطبيق ممتاز بس لو فى امكانية يتيح لمستخدم...       1\n",
              " 4      سيء جدا ، اسعار رسوم التوصيل لا تمت للواقع ب ص...      -1\n",
              " ...                                                  ...     ...\n",
              " 32031  التطبيق اصبح سيء للغايه نقوم بطلب لا يتم وصول ...      -1\n",
              " 32032                                         y love you       1\n",
              " 32033      الباقه بتخلص وبشحن مرتين باقه اضافيه ١٠٠ جنيه      -1\n",
              " 32034  تطبيق فاشل وصلني الطلب ناقص ومش ينفع اعمل حاجة...      -1\n",
              " 32035                                    ليش ما يفتح معي       1\n",
              " \n",
              " [32036 rows x 2 columns],\n",
              "        ID                                 review_description\n",
              " 0       1  اهنئكم على خدمه العملاء في المحادثه المباشره م...\n",
              " 1       2  ممتاز جدا ولكن اتمنى ان تكون هناك بعض المسابقا...\n",
              " 2       3    كل محملته يقول تم ايقاف حطيت2 عشان تسوون الخطاء\n",
              " 3       4                                            شغل طيب\n",
              " 4       5                                         بعد ماجربت\n",
              " ..    ...                                                ...\n",
              " 995   996                                              يستهل\n",
              " 996   997                             خدمة سيئة بكل المعايير\n",
              " 997   998                                      لؤي٠٣٣٢لؤ٣٤٣س\n",
              " 998   999                     تطبيق غير صادق ف خصم الكوبونات\n",
              " 999  1000  تأخير + الموظفين غير مؤهلة .للأسف انا استخدم ط...\n",
              " \n",
              " [1000 rows x 2 columns])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[\"review_description\"] = train_data[\"review_description\"].astype(str)\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].astype(str)\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "test_data[\"review_description\"]  =test_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "\n",
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_dl0cFzSb3a"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTtCOhKKUWWZ",
        "outputId": "62e787ea-2b5d-4212-c487-54193f0e0cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "for letter in '#.][!XR':\n",
        "    train_data[\"review_description\"] = train_data[\"review_description\"].astype(str).str.replace(letter,'')\n",
        "    test_data[\"review_description\"] = test_data[\"review_description\"].astype(str).str.replace(letter,'')\n",
        "\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(train_data[\"review_description\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zHknkvpPVlq6"
      },
      "outputs": [],
      "source": [
        "def translate_EtoA(english_text):\n",
        "\n",
        "    translator = Translator(service_urls=['translate.google.com'],to_lang=\"ar\")\n",
        "    translation = translator.translate(english_text)\n",
        "    arabic_text = translation\n",
        "\n",
        "    return arabic_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fQVPkzOFWDeE"
      },
      "outputs": [],
      "source": [
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tSLj8EIOWDUm"
      },
      "outputs": [],
      "source": [
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uemGp34sWIhZ"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('arabic')) - {'نعم','لا', 'ليس', 'ليست', 'مش', 'ما','غير','أقبل','ليس','ليسا','ليست','ليستا','ليسوا','لست','لستم','لستما','لستن','لسن','لسنا','واو'}\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return \" \".join([word for word in tokens if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AWTIfQ8VWK5b"
      },
      "outputs": [],
      "source": [
        "def build_emoji_dictionary(csv_file):\n",
        "    emoji_dict = {}\n",
        "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            emoji = row[0]\n",
        "            text = row[1]\n",
        "            emoji_dict[emoji] = text\n",
        "    return emoji_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4he6tggMWO0E"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "\n",
        "def replace_emojis(emoji_dict, sentence):\n",
        "    # Convert emojis in the sentence to their textual descriptions\n",
        "    sentence = emoji.demojize(sentence)\n",
        "    \n",
        "    # Iterate through the emoji dictionary to replace with the custom format\n",
        "    for key, value in emoji_dict.items():\n",
        "        sentence = sentence.replace(':' + key + ':', '*' + value + '* ')\n",
        "    \n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SQ-H6KhXWUmy"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vh_XTkW2WYgq"
      },
      "outputs": [],
      "source": [
        "def decode_emojis(text):\n",
        "    decoded_text = emojis.decode(text)\n",
        "    return decoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2FmAqQAVWcaa"
      },
      "outputs": [],
      "source": [
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2pGkQJa5Wj2W"
      },
      "outputs": [],
      "source": [
        "def remove_diactrics(text):\n",
        "    return araby.strip_diacritics(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "yEb10eojWjzH"
      },
      "outputs": [],
      "source": [
        "def processPost(text,emoji_dict):\n",
        "    text = re.sub(r\"\\s+\", \" \", str(text).strip())\n",
        "    text = re.sub('@[^\\s]+', ' ', text)\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    text= remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = replace_emojis(emoji_dict, text)\n",
        "    text= remove_punctuations(text)\n",
        "    text = remove_repeating_char(text)\n",
        "    text = remove_stop_words(text)\n",
        "    text = remove_diactrics(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI0oxSneXRgc",
        "outputId": "d811b0de-9937-4cfc-ff32-33ad5de40fdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "import emoji\n",
        "\n",
        "emoji_dict = build_emoji_dictionary(r\"C:\\Users\\me513\\Downloads\\Depi-20241020T181025Z-001\\Depi\\emojis.csv\")\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(lambda x: processPost(x,emoji_dict))\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].apply(lambda x: processPost(x,emoji_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXu31S2KXRdJ",
        "outputId": "3263acff-384c-4b56-abe9-f9a8e74ff784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data 247\n",
            "test data 6\n"
          ]
        }
      ],
      "source": [
        "train_data.replace('', np.nan, inplace=True)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"train data\",train_data[\"review_description\"].isnull().sum())\n",
        "\n",
        "test_data.replace('', np.nan, inplace=True)\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"test data\", test_data[\"review_description\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ooUxS7UqXxPH"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(str)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(tokenizer.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "r6YM5gMgX04N"
      },
      "outputs": [],
      "source": [
        "def stem(text):\n",
        "    stemmed = []\n",
        "    for word in text:\n",
        "        stemmed.append(stemmer.stem(word))\n",
        "    return stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Dg9VNdnHX2vq"
      },
      "outputs": [],
      "source": [
        "def engstem(text):\n",
        "    e_stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed = []\n",
        "    for word in text:\n",
        "        stemmed.append(e_stemmer.stem(word))\n",
        "    return stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Es986BuiXGZl",
        "outputId": "7ed20d42-0a74-49b3-cdb1-8be835983efb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        شركه زباله سواقين بتبرشم مفيش حتي رقم لشكاوي ا...\n",
              "1        خدمه الدفع طريق الكي نت توقفت عندي اصبح فقط ال...\n",
              "2        تطبيق غبي جاري حذفه عاملين اكواد خصم نستخدمها ...\n",
              "3        فعلا تطبيق متاز امكانيه يتيح لمستخدم التطبيق ا...\n",
              "4              سيء جدا اسعار رسوم التوصيل لا تمت لواقع صله\n",
              "                               ...                        \n",
              "32031    التطبيق اصبح سيء لغايه نقوم بطلب لا يتم وصول ا...\n",
              "32032                                           y love you\n",
              "32033                 الباقه بتخلص وبشحن مرتين باقه اضافيه\n",
              "32034    تطبيق فاشل وصلني الطلب ناقص ومش ينفع اعمل حاجه...\n",
              "32035                                      ليش ما يفتح معي\n",
              "Name: review_description, Length: 32036, dtype: object"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemmer = ISRIStemmer()\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(engstem)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(lambda x: ' '.join(x))\n",
        "\n",
        "train_data[\"review_description\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKpDe64BTdV_"
      },
      "source": [
        "# Tokenization and Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "tyQoBPyvYMt3"
      },
      "outputs": [],
      "source": [
        "X_train = train_data['review_description']\n",
        "y_train = train_data['rating']\n",
        "x_test = test_data['review_description'].astype(str).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ujd_F5MXpVav"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "max_len = max([len(x) for x in X_tokenized])\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_tokenized_test, maxlen=max_len, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "QdKa8layYZxJ"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train)\n",
        "y_train_encoded = y_train + 1\n",
        "y_train_encoded = to_categorical(y_train, num_classes=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ6o-5H5cOG4"
      },
      "source": [
        "# Predict the sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-MBmwr_yFfr4"
      },
      "outputs": [],
      "source": [
        "def prediction_test(model):\n",
        "  predictions = model.predict(X_test_padded)\n",
        "  predicted_classes = predictions.argmax(axis=-1)\n",
        "  predicted_classes = np.vectorize({2: -1, 1: 1, 0: 0}.get)(predicted_classes)  # To shift (0, 1, 2) to (-1, 0, 1)\n",
        "  return predicted_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-_Vev06UBo-"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "oeMW7UlQasoG"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim, rate=0.1):\n",
        "        # Call the constructor of the parent class (tf.keras.Model)\n",
        "        super().__init__()  # This line is crucial for proper initialization\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.attention = tfa.layers.MultiHeadAttention(head_size=num_heads, num_heads=num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(d_model),\n",
        "        ])\n",
        "\n",
        "        self.rnn_layer = layers.GRU(16, return_sequences=True)\n",
        "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        self.dense = layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1], self.embedding.output_dim])\n",
        "\n",
        "        attention_output = self.attention([x, x, x])\n",
        "        x = x + attention_output\n",
        "        x = self.rnn_layer(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = x + attention_output\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.pooling(x)\n",
        "        return self.dense(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "X8VdrxjFZDWi",
        "outputId": "c7f85e90-06df-4c41-b66f-1aed984bf2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\me513\\anaconda3\\envs\\gp\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\me513\\anaconda3\\envs\\gp\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:301: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From c:\\Users\\me513\\anaconda3\\envs\\gp\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\me513\\anaconda3\\envs\\gp\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "201/201 [==============================] - 101s 487ms/step - loss: 0.7221 - accuracy: 0.6914 - val_loss: 0.5931 - val_accuracy: 0.7672\n",
            "Epoch 2/10\n",
            "201/201 [==============================] - 97s 480ms/step - loss: 0.4502 - accuracy: 0.8469 - val_loss: 0.4641 - val_accuracy: 0.8393\n",
            "Epoch 3/10\n",
            "201/201 [==============================] - 97s 482ms/step - loss: 0.3231 - accuracy: 0.8959 - val_loss: 0.5070 - val_accuracy: 0.8276\n",
            "Epoch 4/10\n",
            "201/201 [==============================] - 97s 481ms/step - loss: 0.2309 - accuracy: 0.9262 - val_loss: 0.5409 - val_accuracy: 0.8238\n",
            "Epoch 5/10\n",
            "201/201 [==============================] - 97s 480ms/step - loss: 0.1626 - accuracy: 0.9518 - val_loss: 0.7294 - val_accuracy: 0.8105\n",
            "Epoch 6/10\n",
            "201/201 [==============================] - 102s 508ms/step - loss: 0.1322 - accuracy: 0.9610 - val_loss: 0.8064 - val_accuracy: 0.7737\n",
            "Epoch 7/10\n",
            "201/201 [==============================] - 103s 515ms/step - loss: 0.1070 - accuracy: 0.9653 - val_loss: 0.7906 - val_accuracy: 0.8002\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1f801407eb0>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "output_dim = 3\n",
        "num_transformer_blocks = 2\n",
        "d_model = 96\n",
        "num_heads = 8\n",
        "ff_dim = 128\n",
        "dropout_rate = 0.1\n",
        "learning_rate = 0.00022491452958964034\n",
        "\n",
        "model = TransformerModel(vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim, rate=dropout_rate)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "model.fit(\n",
        "    X_padded, y_train_encoded,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save only the weights\n",
        "model.save_weights('transformer_weights.h5')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
